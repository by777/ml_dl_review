{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 现代卷积神经网络"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## AlexNet\n",
    "![AlexNet](imgs/AlexNet.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "from torchsummary import summary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "\\lfloor (n_h - k_h + p_h + s_h) / s_h \\rfloor \\times \\lfloor (n_w - k_w + p_w + s_w) / s_w \\rfloor\n",
    "$$\n",
    "\n",
    "$$(n_h - k_h + p_h + 1)  \\times （n_w - k_w + p_w + 1）$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    # 输入 1 * 224 * 224\n",
    "    # 注意下面计算shape时 padding * 2\n",
    "\n",
    "    # floor 224 - 11 + 1*2 + 4 / 4 =  [224-11+6]/4 = 219 / 4 = 54\n",
    "    nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(),\n",
    "\n",
    "    # 54 - 3 + 2 / 2 = 53 / 2 = 26\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "\n",
    "    # 26 - 5 + 2*2 + 1 = 26\n",
    "    nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(),\n",
    "\n",
    "    # 26 - 3 + 2 / 2 = 12\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "\n",
    "    # 12 - 3 + 1 * 2 + 1 = 12\n",
    "    nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(),\n",
    "\n",
    "    # 12 - 3 + 1 * 2 + 1 = 12\n",
    "    nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(),\n",
    "\n",
    "    # 12 - 3 + 1 * 2 + 1 = 12\n",
    "    nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(),\n",
    "\n",
    "    # （12 - 3 + 2） / 2 = 5\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "\n",
    "    # 256 * 5 * 5 = 4600\n",
    "    nn.Flatten(),\n",
    "    # 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合\n",
    "    nn.Linear(6400, 4096), nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(4096, 4096), nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    # 最后是输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000\n",
    "    nn.Linear(4096, 10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 96, 54, 54]          11,712\n",
      "              ReLU-2           [-1, 96, 54, 54]               0\n",
      "         MaxPool2d-3           [-1, 96, 26, 26]               0\n",
      "            Conv2d-4          [-1, 256, 26, 26]         614,656\n",
      "              ReLU-5          [-1, 256, 26, 26]               0\n",
      "         MaxPool2d-6          [-1, 256, 12, 12]               0\n",
      "            Conv2d-7          [-1, 384, 12, 12]         885,120\n",
      "              ReLU-8          [-1, 384, 12, 12]               0\n",
      "            Conv2d-9          [-1, 384, 12, 12]       1,327,488\n",
      "             ReLU-10          [-1, 384, 12, 12]               0\n",
      "           Conv2d-11          [-1, 256, 12, 12]         884,992\n",
      "             ReLU-12          [-1, 256, 12, 12]               0\n",
      "        MaxPool2d-13            [-1, 256, 5, 5]               0\n",
      "          Flatten-14                 [-1, 6400]               0\n",
      "           Linear-15                 [-1, 4096]      26,218,496\n",
      "             ReLU-16                 [-1, 4096]               0\n",
      "          Dropout-17                 [-1, 4096]               0\n",
      "           Linear-18                 [-1, 4096]      16,781,312\n",
      "             ReLU-19                 [-1, 4096]               0\n",
      "          Dropout-20                 [-1, 4096]               0\n",
      "           Linear-21                   [-1, 10]          40,970\n",
      "================================================================\n",
      "Total params: 46,764,746\n",
      "Trainable params: 46,764,746\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 10.22\n",
      "Params size (MB): 178.39\n",
      "Estimated Total Size (MB): 188.81\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(net, input_size=(1,224,224))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d output shape:\t torch.Size([1, 96, 54, 54])\n",
      "ReLU output shape:\t torch.Size([1, 96, 54, 54])\n",
      "MaxPool2d output shape:\t torch.Size([1, 96, 26, 26])\n",
      "Conv2d output shape:\t torch.Size([1, 256, 26, 26])\n",
      "ReLU output shape:\t torch.Size([1, 256, 26, 26])\n",
      "MaxPool2d output shape:\t torch.Size([1, 256, 12, 12])\n",
      "Conv2d output shape:\t torch.Size([1, 384, 12, 12])\n",
      "ReLU output shape:\t torch.Size([1, 384, 12, 12])\n",
      "Conv2d output shape:\t torch.Size([1, 384, 12, 12])\n",
      "ReLU output shape:\t torch.Size([1, 384, 12, 12])\n",
      "Conv2d output shape:\t torch.Size([1, 256, 12, 12])\n",
      "ReLU output shape:\t torch.Size([1, 256, 12, 12])\n",
      "MaxPool2d output shape:\t torch.Size([1, 256, 5, 5])\n",
      "Flatten output shape:\t torch.Size([1, 6400])\n",
      "Linear output shape:\t torch.Size([1, 4096])\n",
      "ReLU output shape:\t torch.Size([1, 4096])\n",
      "Dropout output shape:\t torch.Size([1, 4096])\n",
      "Linear output shape:\t torch.Size([1, 4096])\n",
      "ReLU output shape:\t torch.Size([1, 4096])\n",
      "Dropout output shape:\t torch.Size([1, 4096])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(1, 1, 224, 224)\n",
    "for layer in net:\n",
    "    X=layer(X)\n",
    "    print(layer.__class__.__name__,'output shape:\\t',X.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# lr, num_epochs = 0.01, 1\n",
    "# d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## VGG\n",
    "![VGG](imgs/VGG.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "def vgg_block(num_convs, in_channels, out_channels):\n",
    "    '''\n",
    "\n",
    "    :param num_convs: 卷积层的数量\n",
    "    :param in_channels: 输入通道的数量\n",
    "    :param out_channels: 输出通道的数量\n",
    "    :return: layers\n",
    "    '''\n",
    "    layers = []\n",
    "    for num in range(num_convs):\n",
    "        layers.append(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "        layers.append(\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        in_channels = out_channels\n",
    "    layers.append(\n",
    "        nn.MaxPool2d(2,stride=2)\n",
    "    )\n",
    "    return nn.Sequential(*layers)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "上面的代码实现了VGG块，\n",
    "其中有超参数变量conv_arch指定了每个VGG块里卷积层的个数，和输出通道数\n",
    "\n",
    "原始的VGG共有5个卷积块，\n",
    "其中前两个块各有一个卷积层，\n",
    "后三个块各包含2个卷积层。\n",
    "\n",
    "第一个模块有64个输出通道，后续输出通道把输出通道数量翻倍。直到512.\n",
    "\n",
    "由于该网络使用\n",
    "2 * 1 + 3 * 2 = 8个卷积层\n",
    "和3个全连接层，\n",
    "因此它通常被称为VGG-11。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:  torch.Size([1, 64, 112, 112])\n",
      "Sequential output shape:  torch.Size([1, 128, 56, 56])\n",
      "Sequential output shape:  torch.Size([1, 256, 28, 28])\n",
      "Sequential output shape:  torch.Size([1, 512, 14, 14])\n",
      "Sequential output shape:  torch.Size([1, 512, 7, 7])\n",
      "Flatten output shape:  torch.Size([1, 25088])\n",
      "Linear output shape:  torch.Size([1, 4096])\n",
      "ReLU output shape:  torch.Size([1, 4096])\n",
      "Dropout output shape:  torch.Size([1, 4096])\n",
      "Linear output shape:  torch.Size([1, 4096])\n",
      "ReLU output shape:  torch.Size([1, 4096])\n",
      "Dropout output shape:  torch.Size([1, 4096])\n",
      "Linear output shape:  torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "conv_arch = ((1, 64), (1, 128), (2, 256), (2,512), (2, 512))\n",
    "\n",
    "def vgg(conv_arch):\n",
    "    conv_blk = []\n",
    "    in_channels = 1\n",
    "\n",
    "    # 卷积层部分\n",
    "    for (num_convs, out_channels) in conv_arch:\n",
    "        conv_blk.append(\n",
    "            vgg_block(num_convs, in_channels, out_channels)\n",
    "        )\n",
    "        in_channels = out_channels\n",
    "    return nn.Sequential(\n",
    "        *conv_blk,\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "        nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),\n",
    "        nn.Linear(4096, 10)\n",
    "    )\n",
    "net = vgg(conv_arch)\n",
    "\n",
    "X = torch.randn(size=(1,1,224,224))\n",
    "for blk in net:\n",
    "    X = blk(X)\n",
    "    print(blk.__class__.__name__, 'output shape: ', X.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "由于VGG-11比AlexNet计算量更大， 因此我们构建一个通道较少的网络， 足够用来训练Fashion-MNIST数据集"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "ratio = 4\n",
    "small_conv_arch = [(pair[0], pair[1] // ratio) for pair in conv_arch]\n",
    "net = vgg(small_conv_arch)\n",
    "\n",
    "# lr, num_epochs, batch_size = 0.05, 1, 128\n",
    "# train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)\n",
    "# d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 网络中的网络NiN\n",
    "\n",
    "AlexNet，VGG等都有一个共同的设计模式：\n",
    "\n",
    "通过一系列的卷积和汇聚层来提取空间结构特征；\n",
    "然后通过全连接层对特征的表征进行处理。\n",
    "\n",
    "它们的最大改进在于如何扩大和加深这两个模块。\n",
    "\n",
    "或者，可以想象在这个过程的早期使用全连接层。\n",
    "然而，使用了全连接层，可能会完全放弃表征的空间结构。\n",
    "\n",
    "网络中的网络（NiN）提供了一种非常简单的解决方案：\n",
    "\n",
    "**在每个像素的通道上分别使用多层感知机**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### NiN块\n",
    "回想一下，卷积层的输入和输出由4维张量组成，\n",
    "张量的每个轴分别对应样本、通道、高度和宽度。\n",
    "\n",
    "NiN的想法是在每个像素位置（针对每个高度和宽度）应用一个全连接层。 如果我们将权重连接到每个空间位置，我们可以将其视为卷积层\n",
    "\n",
    "![NiN](./imgs/NiN.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def nin_block(in_channels, out_channels, kernel_size, strides, padding):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels,out_channels,kernel_size,strides,padding),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU(),\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU()\n",
    "    )\n",
    "\n",
    "net = nn.Sequential(\n",
    "    # 输入\n",
    "    nin_block(1, 96, kernel_size=11, strides=4, padding=0),\n",
    "    nn.MaxPool2d(3, stride=2),\n",
    "    nin_block(96, 256, kernel_size=5, strides=1, padding=2),\n",
    "    nn.MaxPool2d(3, stride=2),\n",
    "    nin_block(256, 384, kernel_size=3, strides=1, padding=1),\n",
    "    nn.MaxPool2d(3, stride=2),\n",
    "    nn.Dropout(0.5),\n",
    "    # 标签类别数是10\n",
    "    nin_block(384, 10, kernel_size=3, strides=1, padding=1),\n",
    "    nn.AdaptiveAvgPool2d((1, 1)),\n",
    "    # 将四维的输出转成二维的输出，其形状为(批量大小,10)\n",
    "    nn.Flatten())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# lr, num_epochs, batch_size = 0.1, 1, 128\n",
    "# train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)\n",
    "# d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 96, 54, 54]          11,712\n",
      "              ReLU-2           [-1, 96, 54, 54]               0\n",
      "            Conv2d-3           [-1, 96, 54, 54]           9,312\n",
      "              ReLU-4           [-1, 96, 54, 54]               0\n",
      "            Conv2d-5           [-1, 96, 54, 54]           9,312\n",
      "              ReLU-6           [-1, 96, 54, 54]               0\n",
      "         MaxPool2d-7           [-1, 96, 26, 26]               0\n",
      "            Conv2d-8          [-1, 256, 26, 26]         614,656\n",
      "              ReLU-9          [-1, 256, 26, 26]               0\n",
      "           Conv2d-10          [-1, 256, 26, 26]          65,792\n",
      "             ReLU-11          [-1, 256, 26, 26]               0\n",
      "           Conv2d-12          [-1, 256, 26, 26]          65,792\n",
      "             ReLU-13          [-1, 256, 26, 26]               0\n",
      "        MaxPool2d-14          [-1, 256, 12, 12]               0\n",
      "           Conv2d-15          [-1, 384, 12, 12]         885,120\n",
      "             ReLU-16          [-1, 384, 12, 12]               0\n",
      "           Conv2d-17          [-1, 384, 12, 12]         147,840\n",
      "             ReLU-18          [-1, 384, 12, 12]               0\n",
      "           Conv2d-19          [-1, 384, 12, 12]         147,840\n",
      "             ReLU-20          [-1, 384, 12, 12]               0\n",
      "        MaxPool2d-21            [-1, 384, 5, 5]               0\n",
      "          Dropout-22            [-1, 384, 5, 5]               0\n",
      "           Conv2d-23             [-1, 10, 5, 5]          34,570\n",
      "             ReLU-24             [-1, 10, 5, 5]               0\n",
      "           Conv2d-25             [-1, 10, 5, 5]             110\n",
      "             ReLU-26             [-1, 10, 5, 5]               0\n",
      "           Conv2d-27             [-1, 10, 5, 5]             110\n",
      "             ReLU-28             [-1, 10, 5, 5]               0\n",
      "AdaptiveAvgPool2d-29             [-1, 10, 1, 1]               0\n",
      "          Flatten-30                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 1,992,166\n",
      "Trainable params: 1,992,166\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 24.20\n",
      "Params size (MB): 7.60\n",
      "Estimated Total Size (MB): 31.99\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(net, input_size=(1,224,224))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# GoogLeNet\n",
    "GoogLeNet吸收了NiN中串联网络的思想，并在此基础上做了改进。\n",
    "这篇论文的一个重点是解决了什么样大小的卷积核最合适的问题。\n",
    "\n",
    "这篇论文的一个观点是，有时使用不同大小的卷积核是有利的。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inception块\n",
    "![inception](imgs/inceptionblk.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "如图，Inception块由4条\n",
    "\n",
    "**并行路径**\n",
    "组成。\n",
    "\n",
    "这4条路径都使用合适的填充来使输入和输出的高宽一致。\n",
    "最后，我们把每条线路的输出在通道维度上连结，\n",
    "构成Inception的输出。\n",
    "\n",
    "在Inception块中，通常调整的超参数是每层输出的通道数。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l\n",
    "\n",
    "class Inception(nn.Module):\n",
    "    '''c1-c4是每条路径的输出通道数'''\n",
    "    def __init__(self, in_channels, c1, c2, c3, c4, **kwargs):\n",
    "        super(Inception, self).__init__()\n",
    "        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)\n",
    "        self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)\n",
    "        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n",
    "        self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)\n",
    "        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)\n",
    "        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)\n",
    "    def forward(self,x):\n",
    "        # 并联的含义\n",
    "        p1 = F.relu(self.p1_1(x))\n",
    "        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n",
    "        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n",
    "        p4 = F.relu(self.p4_2(self.p4_1(x)))\n",
    "        # 并联，0为批次，1为通道，2为H，3为W\n",
    "        return torch.cat((p1,p2,p3,p4),dim=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "GoogLeNet共使用9个Inception块和全局平均汇聚层的堆叠来生成其估计值。\n",
    "\n",
    "Inception块之间的最大汇聚层可以降低维度。\n",
    "\n",
    "第一个模块类似于AlexNet和LeNet，\n",
    "Inception块的组合从VGG继承，\n",
    "\n",
    "**全局平均汇聚层避免了在最后使用全连接层**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 补充-全局平均汇聚层避免了在最后使用全连接层\n",
    "\n",
    "+ 全局平均池化代替全连接层虽然可以减少模型的参数量，防止模型发生过拟合，但不利于模型的迁移学习，\n",
    "+ 而全连接层则可以更好的进行迁移学习，因为它的参数调整很大一部分是在全连接层中，迁移的时候虽然卷积层的参数也会调整，但是相对来说要小很多\n",
    "\n",
    "参考链接\n",
    "+ [为什么为什么全局平均池化层有用，为什么可以替代全连接层？ - 刘冬煜的回答 - 知乎](https://www.zhihu.com/question/373188099/answer/1026457443)\n",
    "+ [全局平均池化代替全连接层，全连接层的作用？](https://blog.csdn.net/m0_45388819/article/details/120683865)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## gooLeNet\n",
    "![googLent](imgs/googLeNet.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "如图，gooLeNet一共使用个Inception块和GAP的堆叠来生成其估计值"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "\\lfloor (n_h - k_h + p_h + s_h) / s_h \\rfloor \\times \\lfloor (n_w - k_w + p_w + s_w) / s_w \\rfloor\n",
    "$$\n",
    "\n",
    "$$(n_h - k_h + p_h + 1)  \\times （n_w - k_w + p_w + 1）$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "b1 = nn.Sequential(\n",
    "    # 输入 1 * 96 * 96\n",
    "\n",
    "    # floor（96 - 7 + 3*2 + 2） / 2 = 97 / 2 = 48\n",
    "    nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "    nn.ReLU(),\n",
    "\n",
    "    # # 48 - 3 + 1*2 + 1 = 48\n",
    "    # nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "    # nn.ReLU(),\n",
    "\n",
    "    # ( 48 - 3 + 1 * 2 + 2 ) / 2 = 24\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    ")\n",
    "# 输出 64 * 24 * 24"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 48, 48]           3,200\n",
      "              ReLU-2           [-1, 64, 48, 48]               0\n",
      "         MaxPool2d-3           [-1, 64, 24, 24]               0\n",
      "================================================================\n",
      "Total params: 3,200\n",
      "Trainable params: 3,200\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.04\n",
      "Forward/backward pass size (MB): 2.53\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 2.58\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(b1, input_size=(1,96,96))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "第二个模块使用两个卷积层：\n",
    "+ 第一个卷积层是64个通道、1 * 1卷积\n",
    "+ 第二个卷积层使用将通道数量乘以3倍的3*3卷积层\n",
    "\n",
    "这对应Inception块中的第二条路径"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "b2 = nn.Sequential(\n",
    "    # 输入 64 * 24 * 24\n",
    "\n",
    "    # 24 - 1 + 1 = 24\n",
    "    nn.Conv2d(64, 64, kernel_size=1),\n",
    "    nn.ReLU(),\n",
    "\n",
    "    # 24 - 3 + 1 + 1 * 2 = 24\n",
    "    nn.Conv2d(64, 192,kernel_size=3, padding=1),\n",
    "    nn.ReLU(),\n",
    "\n",
    "    # (24 - 3 + 1 * 2 + 2) / 2 = 12\n",
    "    nn.MaxPool2d(kernel_size=3,padding=1, stride=2)\n",
    ")\n",
    "# 输出 192 * 12 * 12"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 24, 24]           4,160\n",
      "              ReLU-2           [-1, 64, 24, 24]               0\n",
      "            Conv2d-3          [-1, 192, 24, 24]         110,784\n",
      "              ReLU-4          [-1, 192, 24, 24]               0\n",
      "         MaxPool2d-5          [-1, 192, 12, 12]               0\n",
      "================================================================\n",
      "Total params: 114,944\n",
      "Trainable params: 114,944\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.14\n",
      "Forward/backward pass size (MB): 2.46\n",
      "Params size (MB): 0.44\n",
      "Estimated Total Size (MB): 3.04\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(b2, input_size=(64, 24, 24))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "b3 = nn.Sequential(\n",
    "    # 输入 192 * 12 * 12\n",
    "\n",
    "    # in_channel是输入通道，c1-c4是每条路径的输出通道数（输入通道数是一样的）\n",
    "\n",
    "    # 输出通道数 = 64 + 128 + 32 + 32 = 256\n",
    "    # Inception block不改变特征图的尺寸\n",
    "    Inception(192, 64, (96, 128), (16, 32), 32),\n",
    "\n",
    "    # 输出通道数 = 128 + 192 + 96 + 64 = 480\n",
    "    Inception(256, 128, (128, 192), (32, 96), 64),\n",
    "\n",
    "    # 改变尺寸一半，输出通道数不变\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    ")\n",
    "# 输出 480 * 6 * 6"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 12, 12]          12,352\n",
      "            Conv2d-2           [-1, 96, 12, 12]          18,528\n",
      "            Conv2d-3          [-1, 128, 12, 12]         110,720\n",
      "            Conv2d-4           [-1, 16, 12, 12]           3,088\n",
      "            Conv2d-5           [-1, 32, 12, 12]          12,832\n",
      "         MaxPool2d-6          [-1, 192, 12, 12]               0\n",
      "            Conv2d-7           [-1, 32, 12, 12]           6,176\n",
      "         Inception-8          [-1, 256, 12, 12]               0\n",
      "            Conv2d-9          [-1, 128, 12, 12]          32,896\n",
      "           Conv2d-10          [-1, 128, 12, 12]          32,896\n",
      "           Conv2d-11          [-1, 192, 12, 12]         221,376\n",
      "           Conv2d-12           [-1, 32, 12, 12]           8,224\n",
      "           Conv2d-13           [-1, 96, 12, 12]          76,896\n",
      "        MaxPool2d-14          [-1, 256, 12, 12]               0\n",
      "           Conv2d-15           [-1, 64, 12, 12]          16,448\n",
      "        Inception-16          [-1, 480, 12, 12]               0\n",
      "        MaxPool2d-17            [-1, 480, 6, 6]               0\n",
      "================================================================\n",
      "Total params: 552,432\n",
      "Trainable params: 552,432\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.11\n",
      "Forward/backward pass size (MB): 2.54\n",
      "Params size (MB): 2.11\n",
      "Estimated Total Size (MB): 4.75\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(b3, input_size=(192, 12, 12))\n",
    "# 观察Inception block不会改变特征图的尺寸"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "b4 = nn.Sequential(\n",
    "    # 输入 480 * 6 * 6\n",
    "\n",
    "    Inception(480, 192, (96, 208), (16, 48), 64),\n",
    "\n",
    "    Inception(512, 160, (112, 224), (24, 64), 64),\n",
    "\n",
    "    Inception(512, 128, (128, 256), (24, 64), 64),\n",
    "\n",
    "    Inception(512, 112, (144, 288), (32, 64), 64),\n",
    "\n",
    "    Inception(528, 256, (160, 320), (32, 128), 128),\n",
    "\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    ")\n",
    "\n",
    "# 输出 832 * 3 * 3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 192, 6, 6]          92,352\n",
      "            Conv2d-2             [-1, 96, 6, 6]          46,176\n",
      "            Conv2d-3            [-1, 208, 6, 6]         179,920\n",
      "            Conv2d-4             [-1, 16, 6, 6]           7,696\n",
      "            Conv2d-5             [-1, 48, 6, 6]          19,248\n",
      "         MaxPool2d-6            [-1, 480, 6, 6]               0\n",
      "            Conv2d-7             [-1, 64, 6, 6]          30,784\n",
      "         Inception-8            [-1, 512, 6, 6]               0\n",
      "            Conv2d-9            [-1, 160, 6, 6]          82,080\n",
      "           Conv2d-10            [-1, 112, 6, 6]          57,456\n",
      "           Conv2d-11            [-1, 224, 6, 6]         226,016\n",
      "           Conv2d-12             [-1, 24, 6, 6]          12,312\n",
      "           Conv2d-13             [-1, 64, 6, 6]          38,464\n",
      "        MaxPool2d-14            [-1, 512, 6, 6]               0\n",
      "           Conv2d-15             [-1, 64, 6, 6]          32,832\n",
      "        Inception-16            [-1, 512, 6, 6]               0\n",
      "           Conv2d-17            [-1, 128, 6, 6]          65,664\n",
      "           Conv2d-18            [-1, 128, 6, 6]          65,664\n",
      "           Conv2d-19            [-1, 256, 6, 6]         295,168\n",
      "           Conv2d-20             [-1, 24, 6, 6]          12,312\n",
      "           Conv2d-21             [-1, 64, 6, 6]          38,464\n",
      "        MaxPool2d-22            [-1, 512, 6, 6]               0\n",
      "           Conv2d-23             [-1, 64, 6, 6]          32,832\n",
      "        Inception-24            [-1, 512, 6, 6]               0\n",
      "           Conv2d-25            [-1, 112, 6, 6]          57,456\n",
      "           Conv2d-26            [-1, 144, 6, 6]          73,872\n",
      "           Conv2d-27            [-1, 288, 6, 6]         373,536\n",
      "           Conv2d-28             [-1, 32, 6, 6]          16,416\n",
      "           Conv2d-29             [-1, 64, 6, 6]          51,264\n",
      "        MaxPool2d-30            [-1, 512, 6, 6]               0\n",
      "           Conv2d-31             [-1, 64, 6, 6]          32,832\n",
      "        Inception-32            [-1, 528, 6, 6]               0\n",
      "           Conv2d-33            [-1, 256, 6, 6]         135,424\n",
      "           Conv2d-34            [-1, 160, 6, 6]          84,640\n",
      "           Conv2d-35            [-1, 320, 6, 6]         461,120\n",
      "           Conv2d-36             [-1, 32, 6, 6]          16,928\n",
      "           Conv2d-37            [-1, 128, 6, 6]         102,528\n",
      "        MaxPool2d-38            [-1, 528, 6, 6]               0\n",
      "           Conv2d-39            [-1, 128, 6, 6]          67,712\n",
      "        Inception-40            [-1, 832, 6, 6]               0\n",
      "        MaxPool2d-41            [-1, 832, 3, 3]               0\n",
      "================================================================\n",
      "Total params: 2,809,168\n",
      "Trainable params: 2,809,168\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.07\n",
      "Forward/backward pass size (MB): 2.56\n",
      "Params size (MB): 10.72\n",
      "Estimated Total Size (MB): 13.34\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(b4, input_size=(480,6,6))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "b5 = nn.Sequential(\n",
    "    # 输入832 * 3 * 3\n",
    "\n",
    "    Inception(832, 256, (160, 320), (32, 128), 128),\n",
    "    Inception(832, 384, (192, 384), (48, 128), 128),\n",
    "\n",
    "    # 输出 1024 * 3 * 3\n",
    "\n",
    "    nn.AdaptiveAvgPool2d((1,1)),\n",
    "    # 池化后的每个通道上的大小是一个1x1的,\n",
    "    # 也就是每个通道上只有一个像素点. (1, 1)表示的outputsize\n",
    "    nn.Flatten()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 256, 3, 3]         213,248\n",
      "            Conv2d-2            [-1, 160, 3, 3]         133,280\n",
      "            Conv2d-3            [-1, 320, 3, 3]         461,120\n",
      "            Conv2d-4             [-1, 32, 3, 3]          26,656\n",
      "            Conv2d-5            [-1, 128, 3, 3]         102,528\n",
      "         MaxPool2d-6            [-1, 832, 3, 3]               0\n",
      "            Conv2d-7            [-1, 128, 3, 3]         106,624\n",
      "         Inception-8            [-1, 832, 3, 3]               0\n",
      "            Conv2d-9            [-1, 384, 3, 3]         319,872\n",
      "           Conv2d-10            [-1, 192, 3, 3]         159,936\n",
      "           Conv2d-11            [-1, 384, 3, 3]         663,936\n",
      "           Conv2d-12             [-1, 48, 3, 3]          39,984\n",
      "           Conv2d-13            [-1, 128, 3, 3]         153,728\n",
      "        MaxPool2d-14            [-1, 832, 3, 3]               0\n",
      "           Conv2d-15            [-1, 128, 3, 3]         106,624\n",
      "        Inception-16           [-1, 1024, 3, 3]               0\n",
      "AdaptiveAvgPool2d-17           [-1, 1024, 1, 1]               0\n",
      "          Flatten-18                 [-1, 1024]               0\n",
      "================================================================\n",
      "Total params: 2,487,536\n",
      "Trainable params: 2,487,536\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.03\n",
      "Forward/backward pass size (MB): 0.41\n",
      "Params size (MB): 9.49\n",
      "Estimated Total Size (MB): 9.93\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(b5, input_size=(832, 3,3))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(1024, 10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 48, 48]           3,200\n",
      "              ReLU-2           [-1, 64, 48, 48]               0\n",
      "         MaxPool2d-3           [-1, 64, 24, 24]               0\n",
      "            Conv2d-4           [-1, 64, 24, 24]           4,160\n",
      "              ReLU-5           [-1, 64, 24, 24]               0\n",
      "            Conv2d-6          [-1, 192, 24, 24]         110,784\n",
      "              ReLU-7          [-1, 192, 24, 24]               0\n",
      "         MaxPool2d-8          [-1, 192, 12, 12]               0\n",
      "            Conv2d-9           [-1, 64, 12, 12]          12,352\n",
      "           Conv2d-10           [-1, 96, 12, 12]          18,528\n",
      "           Conv2d-11          [-1, 128, 12, 12]         110,720\n",
      "           Conv2d-12           [-1, 16, 12, 12]           3,088\n",
      "           Conv2d-13           [-1, 32, 12, 12]          12,832\n",
      "        MaxPool2d-14          [-1, 192, 12, 12]               0\n",
      "           Conv2d-15           [-1, 32, 12, 12]           6,176\n",
      "        Inception-16          [-1, 256, 12, 12]               0\n",
      "           Conv2d-17          [-1, 128, 12, 12]          32,896\n",
      "           Conv2d-18          [-1, 128, 12, 12]          32,896\n",
      "           Conv2d-19          [-1, 192, 12, 12]         221,376\n",
      "           Conv2d-20           [-1, 32, 12, 12]           8,224\n",
      "           Conv2d-21           [-1, 96, 12, 12]          76,896\n",
      "        MaxPool2d-22          [-1, 256, 12, 12]               0\n",
      "           Conv2d-23           [-1, 64, 12, 12]          16,448\n",
      "        Inception-24          [-1, 480, 12, 12]               0\n",
      "        MaxPool2d-25            [-1, 480, 6, 6]               0\n",
      "           Conv2d-26            [-1, 192, 6, 6]          92,352\n",
      "           Conv2d-27             [-1, 96, 6, 6]          46,176\n",
      "           Conv2d-28            [-1, 208, 6, 6]         179,920\n",
      "           Conv2d-29             [-1, 16, 6, 6]           7,696\n",
      "           Conv2d-30             [-1, 48, 6, 6]          19,248\n",
      "        MaxPool2d-31            [-1, 480, 6, 6]               0\n",
      "           Conv2d-32             [-1, 64, 6, 6]          30,784\n",
      "        Inception-33            [-1, 512, 6, 6]               0\n",
      "           Conv2d-34            [-1, 160, 6, 6]          82,080\n",
      "           Conv2d-35            [-1, 112, 6, 6]          57,456\n",
      "           Conv2d-36            [-1, 224, 6, 6]         226,016\n",
      "           Conv2d-37             [-1, 24, 6, 6]          12,312\n",
      "           Conv2d-38             [-1, 64, 6, 6]          38,464\n",
      "        MaxPool2d-39            [-1, 512, 6, 6]               0\n",
      "           Conv2d-40             [-1, 64, 6, 6]          32,832\n",
      "        Inception-41            [-1, 512, 6, 6]               0\n",
      "           Conv2d-42            [-1, 128, 6, 6]          65,664\n",
      "           Conv2d-43            [-1, 128, 6, 6]          65,664\n",
      "           Conv2d-44            [-1, 256, 6, 6]         295,168\n",
      "           Conv2d-45             [-1, 24, 6, 6]          12,312\n",
      "           Conv2d-46             [-1, 64, 6, 6]          38,464\n",
      "        MaxPool2d-47            [-1, 512, 6, 6]               0\n",
      "           Conv2d-48             [-1, 64, 6, 6]          32,832\n",
      "        Inception-49            [-1, 512, 6, 6]               0\n",
      "           Conv2d-50            [-1, 112, 6, 6]          57,456\n",
      "           Conv2d-51            [-1, 144, 6, 6]          73,872\n",
      "           Conv2d-52            [-1, 288, 6, 6]         373,536\n",
      "           Conv2d-53             [-1, 32, 6, 6]          16,416\n",
      "           Conv2d-54             [-1, 64, 6, 6]          51,264\n",
      "        MaxPool2d-55            [-1, 512, 6, 6]               0\n",
      "           Conv2d-56             [-1, 64, 6, 6]          32,832\n",
      "        Inception-57            [-1, 528, 6, 6]               0\n",
      "           Conv2d-58            [-1, 256, 6, 6]         135,424\n",
      "           Conv2d-59            [-1, 160, 6, 6]          84,640\n",
      "           Conv2d-60            [-1, 320, 6, 6]         461,120\n",
      "           Conv2d-61             [-1, 32, 6, 6]          16,928\n",
      "           Conv2d-62            [-1, 128, 6, 6]         102,528\n",
      "        MaxPool2d-63            [-1, 528, 6, 6]               0\n",
      "           Conv2d-64            [-1, 128, 6, 6]          67,712\n",
      "        Inception-65            [-1, 832, 6, 6]               0\n",
      "        MaxPool2d-66            [-1, 832, 3, 3]               0\n",
      "           Conv2d-67            [-1, 256, 3, 3]         213,248\n",
      "           Conv2d-68            [-1, 160, 3, 3]         133,280\n",
      "           Conv2d-69            [-1, 320, 3, 3]         461,120\n",
      "           Conv2d-70             [-1, 32, 3, 3]          26,656\n",
      "           Conv2d-71            [-1, 128, 3, 3]         102,528\n",
      "        MaxPool2d-72            [-1, 832, 3, 3]               0\n",
      "           Conv2d-73            [-1, 128, 3, 3]         106,624\n",
      "        Inception-74            [-1, 832, 3, 3]               0\n",
      "           Conv2d-75            [-1, 384, 3, 3]         319,872\n",
      "           Conv2d-76            [-1, 192, 3, 3]         159,936\n",
      "           Conv2d-77            [-1, 384, 3, 3]         663,936\n",
      "           Conv2d-78             [-1, 48, 3, 3]          39,984\n",
      "           Conv2d-79            [-1, 128, 3, 3]         153,728\n",
      "        MaxPool2d-80            [-1, 832, 3, 3]               0\n",
      "           Conv2d-81            [-1, 128, 3, 3]         106,624\n",
      "        Inception-82           [-1, 1024, 3, 3]               0\n",
      "AdaptiveAvgPool2d-83           [-1, 1024, 1, 1]               0\n",
      "          Flatten-84                 [-1, 1024]               0\n",
      "           Linear-85                   [-1, 10]          10,250\n",
      "================================================================\n",
      "Total params: 5,977,530\n",
      "Trainable params: 5,977,530\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.04\n",
      "Forward/backward pass size (MB): 10.50\n",
      "Params size (MB): 22.80\n",
      "Estimated Total Size (MB): 33.34\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(net, input_size=(1,96,96))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# lr, num_epochs, batch_size = 0.1, 1, 128\n",
    "# train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)\n",
    "# d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 批量规范化"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "训练深层神经网络是十分困难的，特别是在较短的时间内使他们收敛更加棘手。\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 为什么需要批量规范化层\n",
    "\n",
    "+ 首先，数据预处理的方式通常会对最终结果产生巨大影响。\n",
    "\n",
    "    我们应用多层感知机来预测房价的例子时，我们的第一步是标准化输入特征，使其平均值为0，方差为1。\n",
    "    直观地说，这种标准化可以很好地与我们的优化器配合使用，因为它可以将参数的量级进行统一。\n",
    "\n",
    "\n",
    "\n",
    "+ 第二，对于典型的多层感知机或卷积神经网络训练时，中间层中的变量可能具有更广的变化范围：\n",
    "\n",
    "    不论是沿着从输入到输出的层，跨同一层中的单元，或是随着时间的推移，模型参数的随着训练更新变幻莫测。\n",
    "    直观地说，我们可能会猜想，如果一个层的可变值是另一层的100倍，这可能需要对学习率进行补偿调整。\n",
    "\n",
    "\n",
    "+ 第三，更深层的网络很复杂，容易过拟合。 这意味着正则化变得更加重要\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "其原理如下：\n",
    "在每次训练迭代中，我们首先规范化输入，\n",
    "\n",
    "即通过减去其均值并除以其标准差，其中两者均基于当前小批量处理。\n",
    "\n",
    "接下来，我们应用比例系数和比例偏移。\n",
    "\n",
    "正是由于这个基于批量统计的标准化，\n",
    "才有了批量规范化的名称。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 请注意\n",
    "\n",
    "1. 如果我们尝试使用大小为1的小批量应用批量规范化，我们将无法学到任何东西。\n",
    "这是因为在减去均值之后，每个隐藏单元将为0。\n",
    "所以，只有使用足够大的小批量，批量规范化这种方法才是有效且稳定的。\n",
    "\n",
    "2. **批量大小的选择可能比没有批量规范化时更重要**\n",
    "也就是说，宁愿不用BN，也不要用很小的batch_size。\n",
    "\n",
    "3. 我们在方差估计值中添加一个小的常量ε > 0，以防止除以0\n",
    "\n",
    "4. 批量规范化层在“训练模式”（通过小批量统计数据规范化）和预测模式（通过数据集统计规范化）中的功能不同。\n",
    "在训练过程中，我们无法得知使用整个数据集来估计平均值和方差，所以只能根据每个小批次的平均值和方差不断训练模型。\n",
    "而在预测模式下，可以根据整个数据集精确计算批量规范化所需的平均值和方差"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "回想一下，批量规范化层和其他层的一个关键区别是，\n",
    "由于批量规范化在完整的小批量上运行，\n",
    "\n",
    "因此我们不能像以前在引入其他层时那样忽略批量大小。\n",
    "\n",
    "我们在下面讨论这两种情况：全连接层和卷积层，他们的批量规范化实现略有不同。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 全连接层\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "通常，我们把批量规范化层置于\n",
    "\n",
    "**全连接层的仿射变换和激活函数之间**。\n",
    "\n",
    "设全连接层的输入为x，权重参数与偏置参数分别的W和b，激活函数为Φ，批量归一化符号为BN。\n",
    "则，使用批量归一化的全连接层的输出计算为：\n",
    "$$\n",
    "h = \\phi(BN(Wx+b))\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "回想一下，均值和方差是在应用变换的“相同”小批量上计算的。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 卷积层"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "同样，对于卷积层，我们可以在卷积层之后和非线性激活函数之前应用批量规范化。\n",
    "\n",
    "当卷积有多个输出通道时，\n",
    "\n",
    "我们需要对这些通道的“每个”输出执行批量规范化，\n",
    "\n",
    "每个通道都有自己的拉伸（scale）和偏移（shift）参数，\n",
    "这两个参数都是标量。\n",
    "\n",
    "假设我们的小批量包含m个样本，并且对于每个通道，卷积的输出具有高度p和宽度q。\n",
    "\n",
    "那么对于卷积层，\n",
    "\n",
    "**我们在每个输出通道的m\\*p\\*q元素上同时执行每个批量标准化**\n",
    "\n",
    "因此，在计算mean和std时，我们会所有空间位置的值，然后在给定通道内应用相同的均值和方差，\n",
    "以便在每个空间位置对值进行规范化。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "但是，\n",
    "正如我们前面提到的，批量规范化在训练模式和预测模式下的行为通常不同。\n",
    "\n",
    "首先，将训练好的模型用于预测时，我们不再需要样本均值中的噪声以及在微批次上估计每个小批次产生的样本方差了。\n",
    "其次，例如，我们可能需要使用我们的模型对逐个样本进行预测。\n",
    "\n",
    "一种常用的方法是通过移动平均估算整个训练数据集的样本均值和方差，并在预测时使用它们得到确定的输出。\n",
    "可见，和暂退法一样，批量规范化层在训练模式和预测模式下的计算结果也是不一样的。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
    "    '''\n",
    "\n",
    "    :param X:\n",
    "    :param gamma:\n",
    "    :param beta:\n",
    "    :param moving_mean:\n",
    "    :param moving_var:\n",
    "    :param eps:\n",
    "    :param momentum:\n",
    "    :return:\n",
    "    '''\n",
    "    # 判断当前模式时训练模式还是预测模式\n",
    "    if not torch.is_grad_enabled():\n",
    "        # 预测推理模式，直接使用传入的移动均值和移动方差\n",
    "        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n",
    "    else:\n",
    "        # 训练模式\n",
    "        assert len(X.shape) in (2, 4)\n",
    "        if len(X.shape) == 2:\n",
    "            # 全连接层的情况，计算特征维上的均值和方差（竖方向的）\n",
    "            mean = X.mean(dim=0)\n",
    "            var = ((X - mean) ** 2).mean(dim=0)\n",
    "        else:\n",
    "            # 使用二维卷积的情况，计算通道维度上的（axis=1）均值与方差（B，C，H，W）\n",
    "            # 这里我们要保持X的形状以便后面广播运算\n",
    "            mean = X.mean(dim=(0, 2, 3),keepdim=True)\n",
    "            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)\n",
    "        # 训练模式下用当前的mean和var做标准化\n",
    "        X_hat = (X - mean) / torch.sqrt(var + eps)\n",
    "        # 更新移动的均值和方差\n",
    "        moving_mean = momentum  * moving_mean + (1. - momentum) * mean\n",
    "        moving_var = momentum * moving_var + (1. - momentum) * var\n",
    "    Y = gamma * X_hat + beta # 缩放和移位\n",
    "    return Y, moving_mean.data, moving_var.data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[2., 3.]]), 2)"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模拟全连接层\n",
    "X = torch.Tensor(\n",
    "    [[1,2],\n",
    "     [3,4]]\n",
    ")\n",
    "\n",
    "X.mean(dim=0, keepdim=True), len(X)\n",
    "# 相当于表格对列求均值"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 3, 2, 2])"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试卷积层\n",
    "X = torch.Tensor([\n",
    "    [ # 批次\n",
    "        [[1,2],[3,4]], # 通道1\n",
    "        [[5,6],[7,8]], # 通道2\n",
    "        [[9,10],[11,12]] # 通道3\n",
    "    ],\n",
    "    [\n",
    "        [[1,2],[3,4]], #通道1\n",
    "        [[5,6],[7,8]], # 通道2\n",
    "        [[9,10],[11,12]] # 通道3\n",
    "    ]\n",
    "])\n",
    "X.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[[ 2.5000]],\n\n         [[ 6.5000]],\n\n         [[10.5000]]]])"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.mean(dim=(0,2,3),keepdim=True) # 相当于每一个通道求均值\n",
    "# 1 + 2 + 3 + 4 + 1 + 2 + 3 + 4 / 8 = 2.5\n",
    "# （% + 6 + 7 + 8 ） * 2 / 8\n",
    "# 相当于一摞扑克牌，（4）个花色为一个样本，求某一个花色的均值"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "现在我们可以创建一个正确的BatchNorm层，\n",
    "这个层将保持适当的参数：拉伸参数gamma和偏移beta。\n",
    "\n",
    "这两个参数将在训练中更新。\n",
    "此外，我们的层将保存均值和方差的移动平均值，以便在模型预测期间随后使用。\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "抛开算法细节，注意我们实现层的设计模式。\n",
    "\n",
    "通常情况，我们用一个单独的函数定义其数学原理，比如batch_norm，然后再将功能集成到一个自定义层中，\n",
    "\n",
    "其代码主要处理数据转移到训练设备，分配和初始化任何必须的变量，跟踪移动均值。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, num_features, num_dims):\n",
    "        \"\"\"\n",
    "\n",
    "        :param num_features: 完全连接层的输出数量或卷积层的输出通道数。\n",
    "        :param num_dims: 2表示完全连接层，4表示卷积层\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if num_dims == 2:\n",
    "            # 只有2维，说明是对全连接BN，[batch_size, n_features]\n",
    "            shape = (1, num_features)\n",
    "        else:\n",
    "            # 是要对卷积BN，[B, C, H, W]\n",
    "            shape = (1, num_features, 1, 1)\n",
    "        # 参与求梯度和迭代的拉伸和偏移参数，分别初始化成1和0\n",
    "        self.gamma = nn.Parameter(torch.ones(shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))\n",
    "        # 非模型参数的变量初始化为0和1\n",
    "        self.moving_mean = torch.zeros(shape)\n",
    "        self.moving_var = torch.ones(shape)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # 如果X不在内存上，将moving_mean和moving_var\n",
    "        # 复制到X所在显存上\n",
    "        if self.moving_mean.device != X.device:\n",
    "            self.moving_mean = self.moving_mean.to(X.device)\n",
    "            self.moving_var = self.moving_var.to(X.device)\n",
    "        # 保存更新过的moving_mean和moving_var\n",
    "        Y, self.moving_mean, self.moving_var = batch_norm(\n",
    "            X, self.gamma, self.beta, self.moving_mean,\n",
    "            self.moving_var, eps=1e-5, momentum=0.9)\n",
    "        return Y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    # 输入 [256, 1, 28, 28]\n",
    "\n",
    "    # 28 - 5 + 1 = 24 -> [256, 6, 24, 24]\n",
    "    nn.Conv2d(1, 6, kernel_size=5),\n",
    "\n",
    "    # [256, 6, 24, 24]\n",
    "    BatchNorm(6, num_dims=4),\n",
    "\n",
    "    nn.Sigmoid(),\n",
    "\n",
    "    # (24 - 2 + 2) / 2 = 12 -> [256, 6, 12, 12]\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "    # 12  - 5 + 1 = 8 -> [256, 16, 8, 8]\n",
    "    nn.Conv2d(6, 16, kernel_size=5),\n",
    "\n",
    "    # [256, 16, 8, 8]\n",
    "    BatchNorm(16, num_dims=4),\n",
    "\n",
    "    nn.Sigmoid(),\n",
    "\n",
    "    # (8 - 2 + 2) / 2 = 4 -> [256, 16, 4, 4]\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "    # [256, 16 * 4 * 4] -> [256, 256]\n",
    "    nn.Flatten(),\n",
    "\n",
    "    # [256, 120]\n",
    "    nn.Linear(16*4*4, 120),\n",
    "\n",
    "    # [256, 120]\n",
    "    BatchNorm(120, num_dims=2),\n",
    "\n",
    "    nn.Sigmoid(),\n",
    "\n",
    "    # [120, 84]\n",
    "    nn.Linear(120, 84),\n",
    "\n",
    "    # [120, 84]\n",
    "    BatchNorm(84, num_dims=2),\n",
    "\n",
    "    nn.Sigmoid(),\n",
    "\n",
    "    # [84, 10]\n",
    "    nn.Linear(84, 10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [256, 6, 24, 24]             156\n",
      "         BatchNorm-2           [256, 6, 24, 24]               0\n",
      "           Sigmoid-3           [256, 6, 24, 24]               0\n",
      "         AvgPool2d-4           [256, 6, 12, 12]               0\n",
      "            Conv2d-5            [256, 16, 8, 8]           2,416\n",
      "         BatchNorm-6            [256, 16, 8, 8]               0\n",
      "           Sigmoid-7            [256, 16, 8, 8]               0\n",
      "         AvgPool2d-8            [256, 16, 4, 4]               0\n",
      "           Flatten-9                 [256, 256]               0\n",
      "           Linear-10                 [256, 120]          30,840\n",
      "        BatchNorm-11                 [256, 120]               0\n",
      "          Sigmoid-12                 [256, 120]               0\n",
      "           Linear-13                  [256, 84]          10,164\n",
      "        BatchNorm-14                  [256, 84]               0\n",
      "          Sigmoid-15                  [256, 84]               0\n",
      "           Linear-16                  [256, 10]             850\n",
      "================================================================\n",
      "Total params: 44,426\n",
      "Trainable params: 44,426\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.77\n",
      "Forward/backward pass size (MB): 30.15\n",
      "Params size (MB): 0.17\n",
      "Estimated Total Size (MB): 31.09\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(net, input_size=(1,28,28), batch_size=256)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "lr, num_epochs, batch_size = 1.0, 1, 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "# for i in train_iter:\n",
    "#     print(i[0].shape)\n",
    "# d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "看从第一个批量规范化层中学到的拉伸参数gamma和偏移参数beta"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "(Parameter containing:\n tensor([[[[1.]],\n \n          [[1.]],\n \n          [[1.]],\n \n          [[1.]],\n \n          [[1.]],\n \n          [[1.]]]], requires_grad=True),\n Parameter containing:\n tensor([[[[0.]],\n \n          [[0.]],\n \n          [[0.]],\n \n          [[0.]],\n \n          [[0.]],\n \n          [[0.]]]], requires_grad=True))"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[1].gamma, net[1].beta"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ResNet"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "随着设计越来越深的网络，\n",
    "深刻理解“新添加的层如何提升网络的性能”变得至关重要。\n",
    "\n",
    "由于\n",
    "只有当较复杂的函数类包含较小的函数类时，\n",
    "我们才能确保提高它们的性能。\n",
    "\n",
    "对于深度神经网络，\n",
    "如果我们能把新添加的层训练成恒等映射（identity function）$f(x)=x$，\n",
    "新模型将和原模型同样有效。\n",
    "并且，由于新模型可能提出更优的解来拟合训练数据集，\n",
    "因此，添加层似乎更容易降低训练误差。\n",
    "\n",
    "因此，残差网络的核心思想是：\n",
    "\n",
    "每个附加层都应该更容易地包含原始函数作为元素之一。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 残差块\n",
    "![残差块](imgs/残差块.png)\n",
    "\n",
    "![残差块](imgs/残差块2.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, input_channels, num_channels, use1x1conv=False, strides=1):\n",
    "        \"\"\"\n",
    "        残差块\n",
    "        :param input_channels:输入通道数\n",
    "        :param num_channels:内层卷积输出通道\n",
    "        :param use1x1conv:是否使用1x1卷积\n",
    "        :param strides:步长\n",
    "        \"\"\"\n",
    "        super(Residual, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1, stride=strides)\n",
    "        self.conv2 = nn.Conv2d(num_channels,num_channels, kernel_size=3, padding=1)\n",
    "        if use1x1conv:\n",
    "            self.conv3 = nn.Conv2d(input_channels, num_channels, kernel_size=1, stride=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
    "\n",
    "    def forward(self,X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        Y += Y\n",
    "        return F.relu(Y)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "如图和上面代码，此代码生成两种类型的残差块，\n",
    "一种使用了1x1卷积调整通道和分辨率，\n",
    "另一种再应用ReLU非线性函数之前，将输入添加到输出。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "下面检查输入和输出一致的情况"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 6, 6]) torch.Size([4, 3, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "blk = Residual(input_channels=3, num_channels=3)\n",
    "X = torch.rand(size=(4,3,6,6))\n",
    "print(X.shape, blk(X).shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们也可以在增加输出通道数的同时，减半输出的高和宽"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 6, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "blk = Residual(input_channels=3, num_channels=6, use1x1conv=True, strides=2)\n",
    "print(blk(X).shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 6, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "blk = Residual(input_channels=3, num_channels=6, use1x1conv=False, strides=2)\n",
    "print(blk(X).shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ResNet模型\n",
    "![ResNet模型](imgs/resnet18.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "# ResNet的前两层跟之前介绍的GoogLeNet中的一样：\n",
    "# 在输出通道数为64、步幅为2的卷积层后，接步幅为2的的最大汇聚层。\n",
    "# 不同之处在于ResNet每个卷积层后增加了批量规范化层。\n",
    "b1 = nn.Sequential(\n",
    "    # 输入1 * 224 * 224\n",
    "\n",
    "    # （224 - 7 + 6 + 2） / 2 = 112\n",
    "    nn.Conv2d(1,64, kernel_size=7, stride=2, padding=3),\n",
    "\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "\n",
    "    # （112 - 3 + 2 + 2） / 2 = 56\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [1, 64, 112, 112]           3,200\n",
      "       BatchNorm2d-2          [1, 64, 112, 112]             128\n",
      "              ReLU-3          [1, 64, 112, 112]               0\n",
      "         MaxPool2d-4            [1, 64, 56, 56]               0\n",
      "================================================================\n",
      "Total params: 3,328\n",
      "Trainable params: 3,328\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 19.91\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 20.11\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(b1, input_size=(1,224, 224), batch_size=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "GoogLeNet在后面添加了4个由Inception组成的模块，\n",
    "ResNet则使用4个由残差块组成的模块，\n",
    "\n",
    "每个模块使用若干个相同输出通道的残差块。\n",
    "\n",
    "第一个模块同输入通道一致，\n",
    "由于之前已经使用了步幅为2的最大汇聚层，\n",
    "所以无需减少高和宽。\n",
    "\n",
    "之后每个模块在第一个模块的基础上把通道数翻倍，高和宽减半。\n",
    "\n",
    "下面我们来实现这个模块，注意第一个模块做了特殊处理。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "def resnet_block(input_channels, num_channels, num_residuals, first_block=False):\n",
    "    \"\"\"\n",
    "    # 输入[batch_size, 64, 56, 56]\n",
    "    :param input_channels:输入通道\n",
    "    :param num_channels:输出通道\n",
    "    :param num_residuals:组合几个残差块\n",
    "    :param first_block:是否是一个模块\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    blk = []\n",
    "    for i in range(num_residuals):\n",
    "        # 如果是每个resnet_block的第1块且不是b2\n",
    "        if i == 0 and not first_block:\n",
    "            blk.append(Residual(input_channels, num_channels, use1x1conv=True, strides=2))\n",
    "        else:\n",
    "            blk.append(Residual(num_channels, num_channels))\n",
    "    return blk"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "# b2 没有添加1x1 卷积，也不改变通道和尺寸\n",
    "b2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True))\n",
    "\n",
    "# b3 通道翻倍，尺寸减半\n",
    "b3 = nn.Sequential(*resnet_block(64, 128, 2))\n",
    "\n",
    "# b4 通道翻倍，尺寸减半\n",
    "b4 = nn.Sequential(*resnet_block(128, 256, 2))\n",
    "\n",
    "# b5 通道翻倍，尺寸减半\n",
    "b5 = nn.Sequential(*resnet_block(256, 512, 2))\n",
    "\n",
    "# 最后，与GoogLeNet一样，在ResNet中加入全局平均汇聚层，以及全连接层输出。\n",
    "net = nn.Sequential(b1, b2, b3, b4, b5,\n",
    "                    # [1, 512, 7, 7]\n",
    "\n",
    "                    # [1, 512, 1, 1]\n",
    "                    nn.AdaptiveAvgPool2d((1,1)),\n",
    "\n",
    "                    # [1, 512]\n",
    "                    nn.Flatten(),\n",
    "                    # [512, 10]\n",
    "                    nn.Linear(512, 10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [1, 64, 56, 56]          36,928\n",
      "       BatchNorm2d-2            [1, 64, 56, 56]             128\n",
      "            Conv2d-3            [1, 64, 56, 56]          36,928\n",
      "       BatchNorm2d-4            [1, 64, 56, 56]             128\n",
      "          Residual-5            [1, 64, 56, 56]               0\n",
      "            Conv2d-6            [1, 64, 56, 56]          36,928\n",
      "       BatchNorm2d-7            [1, 64, 56, 56]             128\n",
      "            Conv2d-8            [1, 64, 56, 56]          36,928\n",
      "       BatchNorm2d-9            [1, 64, 56, 56]             128\n",
      "         Residual-10            [1, 64, 56, 56]               0\n",
      "================================================================\n",
      "Total params: 148,224\n",
      "Trainable params: 148,224\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.77\n",
      "Forward/backward pass size (MB): 15.31\n",
      "Params size (MB): 0.57\n",
      "Estimated Total Size (MB): 16.64\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(b2, input_size=(64, 56, 56), batch_size=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [1, 128, 28, 28]          73,856\n",
      "       BatchNorm2d-2           [1, 128, 28, 28]             256\n",
      "            Conv2d-3           [1, 128, 28, 28]         147,584\n",
      "       BatchNorm2d-4           [1, 128, 28, 28]             256\n",
      "            Conv2d-5           [1, 128, 28, 28]           8,320\n",
      "          Residual-6           [1, 128, 28, 28]               0\n",
      "            Conv2d-7           [1, 128, 28, 28]         147,584\n",
      "       BatchNorm2d-8           [1, 128, 28, 28]             256\n",
      "            Conv2d-9           [1, 128, 28, 28]         147,584\n",
      "      BatchNorm2d-10           [1, 128, 28, 28]             256\n",
      "         Residual-11           [1, 128, 28, 28]               0\n",
      "================================================================\n",
      "Total params: 525,952\n",
      "Trainable params: 525,952\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.77\n",
      "Forward/backward pass size (MB): 8.42\n",
      "Params size (MB): 2.01\n",
      "Estimated Total Size (MB): 11.19\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(b3, input_size=(64, 56, 56), batch_size=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [1, 256, 14, 14]         295,168\n",
      "       BatchNorm2d-2           [1, 256, 14, 14]             512\n",
      "            Conv2d-3           [1, 256, 14, 14]         590,080\n",
      "       BatchNorm2d-4           [1, 256, 14, 14]             512\n",
      "            Conv2d-5           [1, 256, 14, 14]          33,024\n",
      "          Residual-6           [1, 256, 14, 14]               0\n",
      "            Conv2d-7           [1, 256, 14, 14]         590,080\n",
      "       BatchNorm2d-8           [1, 256, 14, 14]             512\n",
      "            Conv2d-9           [1, 256, 14, 14]         590,080\n",
      "      BatchNorm2d-10           [1, 256, 14, 14]             512\n",
      "         Residual-11           [1, 256, 14, 14]               0\n",
      "================================================================\n",
      "Total params: 2,100,480\n",
      "Trainable params: 2,100,480\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.38\n",
      "Forward/backward pass size (MB): 4.21\n",
      "Params size (MB): 8.01\n",
      "Estimated Total Size (MB): 12.61\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(b4, input_size=(128, 28, 28), batch_size=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1             [1, 512, 7, 7]       1,180,160\n",
      "       BatchNorm2d-2             [1, 512, 7, 7]           1,024\n",
      "            Conv2d-3             [1, 512, 7, 7]       2,359,808\n",
      "       BatchNorm2d-4             [1, 512, 7, 7]           1,024\n",
      "            Conv2d-5             [1, 512, 7, 7]         131,584\n",
      "          Residual-6             [1, 512, 7, 7]               0\n",
      "            Conv2d-7             [1, 512, 7, 7]       2,359,808\n",
      "       BatchNorm2d-8             [1, 512, 7, 7]           1,024\n",
      "            Conv2d-9             [1, 512, 7, 7]       2,359,808\n",
      "      BatchNorm2d-10             [1, 512, 7, 7]           1,024\n",
      "         Residual-11             [1, 512, 7, 7]               0\n",
      "================================================================\n",
      "Total params: 8,395,264\n",
      "Trainable params: 8,395,264\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 2.11\n",
      "Params size (MB): 32.03\n",
      "Estimated Total Size (MB): 34.32\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(b5, input_size=(256, 14, 14), batch_size=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      " AdaptiveAvgPool2d-1             [1, 512, 1, 1]               0\n",
      "           Flatten-2                   [1, 512]               0\n",
      "            Linear-3                    [1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 5,130\n",
      "Trainable params: 5,130\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.10\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.02\n",
      "Estimated Total Size (MB): 0.12\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "tmp = nn.Sequential(nn.AdaptiveAvgPool2d((1,1)),\n",
    "                    nn.Flatten(),\n",
    "                    nn.Linear(512, 10))\n",
    "summary(tmp, input_size=(512, 7, 7), batch_size=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "# lr, num_epochs, batch_size = 0.05, 10, 256\n",
    "# train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)\n",
    "# d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 稠密连接网络"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "回想一下。任意函数的泰勒展开式，\n",
    "\n",
    "它把函数分解成越来越高阶的项，在x->0时，\n",
    "\n",
    "![泰勒展开](imgs/泰勒展开.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "同样，把ResNet函数展开：\n",
    "$$\n",
    "f(x) = x + g(x)\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "也就是说，ResNet把$f$分解成了两部分：\n",
    "\n",
    "一个简单的线性项和一个复杂的非线性项。\n",
    "\n",
    "那么再向前扩展一步，如果把$f$扩展成超过两部分的信息呢？\n",
    "\n",
    "一种方案就是DenseNet。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![DenseAndRes](imgs/DenseNet与ResNet.png)\n",
    "\n",
    "ResNet（左）与 DenseNet（右）在跨层连接上的主要区别：使用相加和使用连结。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ResNet与DenseNet的关键区别在于，\n",
    "DenseNet的输出时连接，而不是ResNet的简单相加。\n",
    "\n",
    "稠密网络主要由2部分构成：\n",
    "稠密块（dense block）和过渡层（transition layer）。\n",
    "前者定义如何连接输入和输出，后者控制通道数量，使其不会过于复杂。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 稠密块体"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "\n",
    "def conv_block(input_channels, num_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.BatchNorm2d(input_channels),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1)\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "一个稠密块由多个卷积块组成，\n",
    "\n",
    "每个卷积块使用相同数量的输出通道。\n",
    "\n",
    "然而，在前向传播中，\n",
    "\n",
    "我们将每个卷积块的输入和输出在通道维度上连结。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [],
   "source": [
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, num_convs, input_channels, num_channels):\n",
    "        \"\"\"\n",
    "        构建DenseBlock，每个DenseBlock由组合卷积构成，并通道数目增加\n",
    "        :param num_convs: 使用几个组合卷积\n",
    "        :param input_channels: 输入通道数\n",
    "        :param num_channels: 输出通道增长率\n",
    "        \"\"\"\n",
    "        super(DenseBlock, self).__init__()\n",
    "        layer = []\n",
    "        for i in range(num_convs):\n",
    "            # print(f'{i}. conv_block({i * num_channels + input_channels, num_channels})')\n",
    "\n",
    "            layer.append(\n",
    "                conv_block(i * num_channels + input_channels, num_channels)\n",
    "            )\n",
    "        self.net = nn.Sequential(*layer)\n",
    "    def forward(self, X):\n",
    "        for blk in self.net:\n",
    "            Y = blk(X)\n",
    "            # 连接通道维度上每个块的输入和输出\n",
    "            # print('X.shape', X.shape, 'Y.shape', Y.shape)\n",
    "            X = torch.cat((X,Y),dim=1)\n",
    "        return X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "下面我们定义一个有2个输出通道为10的DenseBlock。\n",
    "\n",
    "使用通道数为3的输入时，\n",
    "我们将会得到通道数为3 + 2 * 10 = 23的输出。\n",
    "\n",
    "卷积块的通道数控制了输出通道数相对于输入通道数的增长，\n",
    "\n",
    "因此也被称为增长率（growth rate）"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([4, 23, 8, 8])"
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2个输出通道为10的DenseBlock，输入通道为3，10就是增长率\n",
    "blk = DenseBlock(2, 3, 10)\n",
    "X = torch.randn(4, 3, 8, 8)\n",
    "Y = blk(X)\n",
    "Y.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "也就是，每次都经过conv_block都会把结果按通道拼接，送入下一个conv_block"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 过渡层\n",
    "由于每个稠密块都会带来通道数的增加，\n",
    "使用过多会产生过于复杂的模型。\n",
    "\n",
    "过渡层可以控制模型的复杂度。\n",
    "\n",
    "它使用1 x 1卷积来减少通道数，\n",
    "并使用步幅为2的平均汇聚层来减半H和W，\n",
    "从而进一步降低模型复杂度。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [],
   "source": [
    "def transition_block(input_channels, num_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.BatchNorm2d(input_channels),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(input_channels, num_channels, kernel_size=1),\n",
    "        nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([4, 10, 4, 4])"
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用通道数为10的过渡层。 此时输出的通道数减为10，高和宽均减半。\n",
    "blk = transition_block(23, 10)\n",
    "blk(Y).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DenseNet模型\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [],
   "source": [
    "b1 = nn.Sequential(\n",
    "    nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
    "    nn.BatchNorm2d(64), nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "# 输出[1, 64, 24, 24]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [1, 64, 48, 48]           3,200\n",
      "       BatchNorm2d-2            [1, 64, 48, 48]             128\n",
      "              ReLU-3            [1, 64, 48, 48]               0\n",
      "         MaxPool2d-4            [1, 64, 24, 24]               0\n",
      "================================================================\n",
      "Total params: 3,328\n",
      "Trainable params: 3,328\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.04\n",
      "Forward/backward pass size (MB): 3.66\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 3.70\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(b1, input_size=(1,96,96), batch_size=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "接下来，类似与ResNet使用4个残差块，DenseNet使用4个稠密块。\n",
    "\n",
    "与ResNet类似，我们可以设置每个稠密块使用多少个卷积层。\n",
    "这里我们设置4，以和resnet18保持一致。\n",
    "\n",
    "稠密块里卷积层通道数（增长率）设置为32，所以每个稠密块将增长128个通道。\n",
    "\n",
    "在每个模块之间，ResNet通过步幅为2的残差块减小高和宽，\n",
    "DenseNet则使用过渡层来减半高和宽，并减半通道数。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [],
   "source": [
    "# num_channels为当前的通道数\n",
    "# # [1, 64, 24, 24]\n",
    "\n",
    "num_channels, growth_rate = 64, 32\n",
    "num_convs_in_dense_blocks = [4, 4, 4, 4]\n",
    "\n",
    "blks = []\n",
    "for i, num_convs in enumerate(num_convs_in_dense_blocks):\n",
    "    # DenseBlock(4,64,32)\n",
    "    blks.append(DenseBlock(num_convs, num_channels, growth_rate))\n",
    "    # 更新上一个稠密块的输出通道数，便于下一轮循环继续构建\n",
    "    num_channels += num_convs * growth_rate\n",
    "    # 在稠密块之间增加一个过渡层\n",
    "    if i != len(num_convs_in_dense_blocks)-1:\n",
    "        blks.append(transition_block(num_channels, num_channels//2))\n",
    "        num_channels = num_channels // 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    # 输入[1,1,96,96]\n",
    "\n",
    "    # [1, 64, 24, 24]\n",
    "    b1,\n",
    "    # [1, 248, 3, 3]\n",
    "    *blks,\n",
    "    # [1, 248, 3, 3]\n",
    "    nn.BatchNorm2d(num_channels), nn.ReLU(),\n",
    "    nn.AdaptiveAvgPool2d((1, 1)),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(num_channels, 10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [1, 64, 48, 48]           3,200\n",
      "       BatchNorm2d-2            [1, 64, 48, 48]             128\n",
      "              ReLU-3            [1, 64, 48, 48]               0\n",
      "         MaxPool2d-4            [1, 64, 24, 24]               0\n",
      "       BatchNorm2d-5            [1, 64, 24, 24]             128\n",
      "              ReLU-6            [1, 64, 24, 24]               0\n",
      "            Conv2d-7            [1, 32, 24, 24]          18,464\n",
      "       BatchNorm2d-8            [1, 96, 24, 24]             192\n",
      "              ReLU-9            [1, 96, 24, 24]               0\n",
      "           Conv2d-10            [1, 32, 24, 24]          27,680\n",
      "      BatchNorm2d-11           [1, 128, 24, 24]             256\n",
      "             ReLU-12           [1, 128, 24, 24]               0\n",
      "           Conv2d-13            [1, 32, 24, 24]          36,896\n",
      "      BatchNorm2d-14           [1, 160, 24, 24]             320\n",
      "             ReLU-15           [1, 160, 24, 24]               0\n",
      "           Conv2d-16            [1, 32, 24, 24]          46,112\n",
      "       DenseBlock-17           [1, 192, 24, 24]               0\n",
      "      BatchNorm2d-18           [1, 192, 24, 24]             384\n",
      "             ReLU-19           [1, 192, 24, 24]               0\n",
      "           Conv2d-20            [1, 96, 24, 24]          18,528\n",
      "        AvgPool2d-21            [1, 96, 12, 12]               0\n",
      "      BatchNorm2d-22            [1, 96, 12, 12]             192\n",
      "             ReLU-23            [1, 96, 12, 12]               0\n",
      "           Conv2d-24            [1, 32, 12, 12]          27,680\n",
      "      BatchNorm2d-25           [1, 128, 12, 12]             256\n",
      "             ReLU-26           [1, 128, 12, 12]               0\n",
      "           Conv2d-27            [1, 32, 12, 12]          36,896\n",
      "      BatchNorm2d-28           [1, 160, 12, 12]             320\n",
      "             ReLU-29           [1, 160, 12, 12]               0\n",
      "           Conv2d-30            [1, 32, 12, 12]          46,112\n",
      "      BatchNorm2d-31           [1, 192, 12, 12]             384\n",
      "             ReLU-32           [1, 192, 12, 12]               0\n",
      "           Conv2d-33            [1, 32, 12, 12]          55,328\n",
      "       DenseBlock-34           [1, 224, 12, 12]               0\n",
      "      BatchNorm2d-35           [1, 224, 12, 12]             448\n",
      "             ReLU-36           [1, 224, 12, 12]               0\n",
      "           Conv2d-37           [1, 112, 12, 12]          25,200\n",
      "        AvgPool2d-38             [1, 112, 6, 6]               0\n",
      "      BatchNorm2d-39             [1, 112, 6, 6]             224\n",
      "             ReLU-40             [1, 112, 6, 6]               0\n",
      "           Conv2d-41              [1, 32, 6, 6]          32,288\n",
      "      BatchNorm2d-42             [1, 144, 6, 6]             288\n",
      "             ReLU-43             [1, 144, 6, 6]               0\n",
      "           Conv2d-44              [1, 32, 6, 6]          41,504\n",
      "      BatchNorm2d-45             [1, 176, 6, 6]             352\n",
      "             ReLU-46             [1, 176, 6, 6]               0\n",
      "           Conv2d-47              [1, 32, 6, 6]          50,720\n",
      "      BatchNorm2d-48             [1, 208, 6, 6]             416\n",
      "             ReLU-49             [1, 208, 6, 6]               0\n",
      "           Conv2d-50              [1, 32, 6, 6]          59,936\n",
      "       DenseBlock-51             [1, 240, 6, 6]               0\n",
      "      BatchNorm2d-52             [1, 240, 6, 6]             480\n",
      "             ReLU-53             [1, 240, 6, 6]               0\n",
      "           Conv2d-54             [1, 120, 6, 6]          28,920\n",
      "        AvgPool2d-55             [1, 120, 3, 3]               0\n",
      "      BatchNorm2d-56             [1, 120, 3, 3]             240\n",
      "             ReLU-57             [1, 120, 3, 3]               0\n",
      "           Conv2d-58              [1, 32, 3, 3]          34,592\n",
      "      BatchNorm2d-59             [1, 152, 3, 3]             304\n",
      "             ReLU-60             [1, 152, 3, 3]               0\n",
      "           Conv2d-61              [1, 32, 3, 3]          43,808\n",
      "      BatchNorm2d-62             [1, 184, 3, 3]             368\n",
      "             ReLU-63             [1, 184, 3, 3]               0\n",
      "           Conv2d-64              [1, 32, 3, 3]          53,024\n",
      "      BatchNorm2d-65             [1, 216, 3, 3]             432\n",
      "             ReLU-66             [1, 216, 3, 3]               0\n",
      "           Conv2d-67              [1, 32, 3, 3]          62,240\n",
      "       DenseBlock-68             [1, 248, 3, 3]               0\n",
      "      BatchNorm2d-69             [1, 248, 3, 3]             496\n",
      "             ReLU-70             [1, 248, 3, 3]               0\n",
      "AdaptiveAvgPool2d-71             [1, 248, 1, 1]               0\n",
      "          Flatten-72                   [1, 248]               0\n",
      "           Linear-73                    [1, 10]           2,490\n",
      "================================================================\n",
      "Total params: 758,226\n",
      "Trainable params: 758,226\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.04\n",
      "Forward/backward pass size (MB): 14.29\n",
      "Params size (MB): 2.89\n",
      "Estimated Total Size (MB): 17.22\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(net, input_size=(1,96,96), batch_size=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [],
   "source": [
    "# lr, num_epochs, batch_size = 0.1, 10, 256\n",
    "# train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)\n",
    "# d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}