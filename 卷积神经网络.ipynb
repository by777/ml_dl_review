{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 6.1. 从全连接层到卷积"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们之前讨论的多层感知机十分适合处理表格数据，\n",
    "其中行对应样本，列对应特征。\n",
    "对于表格数据，我们寻找的模式可能涉及特征之间的交互，\n",
    "但是我们不能预先假设任何与特征交互相关的先验结构。\n",
    "\n",
    "此时，多层感知机可能是最好的选择，\n",
    "但是，对于高维感知数据，这种缺少结构的网络可能变得不实用。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "例如，对于猫狗分类，假设我们有一个足够充分的数据集，\n",
    "每个图片具有百万级像素，这意味着每次输入都有一百万个维度。\n",
    "及时把隐藏层维度降低到1000，这个全连接层依旧有\n",
    "10_000_000 * 10_000个参数。\n",
    "\n",
    "想要训练这个模型将不可实现。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6.1.1. 不变性"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "想象一下，假设你想从一张图片中找到某个物体。\n",
    "合理的假设是：无论哪种方法找到这个物体，都应该和物体的位置无关。\n",
    "理想情况下，我们的系统应该能够利用常识：猪通常不在天上飞，飞机通常不在水里游泳。\n",
    "但是，如果一只猪出现在图片顶部，我们还是应该认出它。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "卷积神经网络正是将“空间不变性（spatial invariance）”的概念系统化，从而基于这个模型使用较少的参数来学习有用的表示。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 互相关操作\n",
    "严格来说，卷积层是个错误的叫法，因为它所表达的运算其实是互相关运算（cross-correlation），而不是卷积运算。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "在二维互相关运算中，卷积窗口从输入张量的左上角开始，从左到右、从上到下滑动。\n",
    "\n",
    "注意，输出大小略小于输入大小。这是因为卷积核的宽度和高度大于1，\n",
    "而卷积核只与图像中每个大小完全适合的位置进行互相关运算。\n",
    "\n",
    "所以，输出大小等于输入大小$n_h, n_w$减去卷积核大小$k_h, k_w$，即\n",
    "$$(n_h-k_h+1, n_w-k_w+1)$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "def corr2d(X, K):  #@save\n",
    "    \"\"\"计算二维互相关运算\n",
    "    @:param X: 输入\n",
    "    @:param K： 卷积核\n",
    "    \"\"\"\n",
    "    h, w = K.shape # 卷积核的高和宽\n",
    "    # 输出Y\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        # print(f'---------当前第{i}行----------')\n",
    "        # 在输入图像上逐行\n",
    "        for j in range(Y.shape[1]):\n",
    "            # 在输入图像上逐列\n",
    "            # print(f'当前第{j}列')\n",
    "            # X[i:i + h, j:j + w]和 K等形状，可以直接相乘\n",
    "            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()\n",
    "    return Y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[19., 25.],\n        [37., 43.]])"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n",
    "K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])\n",
    "corr2d(X, K)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![互相关](imgs/互相关.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 卷积层"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "卷积层对输入和卷积核进行互相关操作，\n",
    "并在添加标量偏置之后产生输出。\n",
    "\n",
    "所以卷积层中两个被训练的参数是卷积核权重和变量偏置，\n",
    "就像我们之前随机初始化全连接层一样，在训练基于卷积层的模型时，我们也随机初始化卷积核权重。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "class Conv2d(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super.__init__()\n",
    "        self.weight = nn.Parameter(torch.rand(kernel_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "    def forward(self, x):\n",
    "        return corr2d(x, self.weight) + self.bias"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 图像中目标的边缘检测\n",
    "\n",
    "如下是卷积层的一个简单应用：通过找到像素变化的位置，来检测图像中不同颜色的边缘。 首先，我们构造一个像素的黑白图像。中间四列为黑色（），其余像素为白色（）。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n        [1., 1., 0., 0., 0., 0., 1., 1.],\n        [1., 1., 0., 0., 0., 0., 1., 1.],\n        [1., 1., 0., 0., 0., 0., 1., 1.],\n        [1., 1., 0., 0., 0., 0., 1., 1.],\n        [1., 1., 0., 0., 0., 0., 1., 1.]])"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones((6,8))\n",
    "X[:, 2:6] = 0\n",
    "X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "接下来，我们构造一个高度为、宽度为的卷积核K。\n",
    "当进行互相关运算时，\n",
    "如果水平相邻的两元素相同，则输出为零，否则输出为非零。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "K = torch.tensor([[1.0,-1.0]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = corr2d(X, K)\n",
    "Y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "输出Y中的1代表从白色到黑色的边缘，-1代表从黑色到白色的边缘，其他情况的输出为0。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "现在我们将输入的二维图像转置，再进行如上的互相关运算。 其输出如下，之前检测到的垂直边缘消失了。 不出所料，这个卷积核K只可以检测垂直边缘，无法检测水平边缘。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]])"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d(X.t(), K)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 学习卷积核"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "如果我们只需寻找黑白边缘，\n",
    "那么以上[1, -1]的边缘检测器足以。\n",
    "然而，当有了更复杂数值的卷积核，\n",
    "或者连续的卷积层时，我们不可能手动设计滤波器。\n",
    "\n",
    "那么我们是否可以学习由X生成Y的卷积核呢？"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "现在我们看看是否通过仅查看“输入-输出”对来学习由X生成Y的卷积核。\n",
    "\n",
    "我们先构造一个卷积层，并将卷积核初始化为随机张量。\n",
    "\n",
    "接下来，在每次迭代中，我们比较Y与卷积层输出的平方误差，\n",
    "\n",
    "然后计算梯度来更新卷积核。\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 1., 0., 0., 0., 0., 1., 1.],\n",
      "          [1., 1., 0., 0., 0., 0., 1., 1.],\n",
      "          [1., 1., 0., 0., 0., 0., 1., 1.],\n",
      "          [1., 1., 0., 0., 0., 0., 1., 1.],\n",
      "          [1., 1., 0., 0., 0., 0., 1., 1.],\n",
      "          [1., 1., 0., 0., 0., 0., 1., 1.]]]]) tensor([[[[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
      "          [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
      "          [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
      "          [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
      "          [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
      "          [ 0.,  1.,  0.,  0.,  0., -1.,  0.]]]])\n",
      "epoch 2， loss 4.586\n",
      "epoch 4， loss 1.472\n",
      "epoch 6， loss 0.535\n",
      "epoch 8， loss 0.207\n",
      "epoch 10， loss 0.083\n"
     ]
    }
   ],
   "source": [
    "# 构造一个二维卷积层，它具有1个输出通道和形状为（1，2）的卷积核\n",
    "conv2d = nn.Conv2d(1,1,kernel_size=(1,2),bias=False)\n",
    "\n",
    "# 这个二维卷积层使用4维输入和输出格式（B，C，H，W）\n",
    "# 其中B=C=1\n",
    "X = X.reshape((1,1,6,8))\n",
    "Y = Y.reshape((1,1,6,7))\n",
    "print(X, Y)\n",
    "lr = 3e-2\n",
    "\n",
    "for i in range(10):\n",
    "    Y_hat = conv2d(X)\n",
    "    l = (Y_hat - Y) ** 2\n",
    "    conv2d.zero_grad()\n",
    "    l.sum().backward()\n",
    "    # 迭代卷积核\n",
    "    conv2d.weight.data[:] -= lr * conv2d.weight.grad\n",
    "    if (i + 1 )% 2 == 0:\n",
    "        print(f'epoch {i+1}， loss {l.sum():.3f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "现在我们来看看我们所学的卷积核的权重张量。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.9637, -1.0228]])"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d.weight.data.reshape((1,2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "细心的你一定会发现，我们学习到的卷积核权重非常接近我们之前定义的卷积核K。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 填充和步幅"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "有时，在我们应用了连续对的卷积之后，我们最终得到的输出远小于输入大小。\n",
    "\n",
    "这是由于卷积核的宽度和高度大于1所导致的。\n",
    "\n",
    "比如，一个（240，240）像素的图像，经过层的卷积后，将减少到（200，200）像素。\n",
    "如此一来，原始图像的边界丢失了许多有用信息。\n",
    "而填充是解决此问题最有效的方法。\n",
    "\n",
    "还有时，我们可能希望大幅降低图像的宽度和高度。\n",
    "例如，如果我们发现原始的输入分辨率十分冗余。步幅则可以在这类情况下提供帮助。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "通常，如果我们添加$p_h$行填充（大约一半在顶部，一半在底部），\n",
    "和$p_w$列填充（一半在左，一半在右），\n",
    "则输出的形状为\n",
    "$$(n_h - k_h + p_h + 1) * （n_w - k_w + p_w + 1）$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "这意味着输出的宽度和高度通常会增加$p_h$和$p_w$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "在许多情况下，我们要设置\n",
    "\n",
    "$$p_h = k_h - 1$$\n",
    "\n",
    "和\n",
    "\n",
    "$$p_w = k_w - 1$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "这样，经过化简，就可以得到输出的形状为$(n_h, n_w)$保持不变了。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "使输入和输出具有相同的高度和宽度。\n",
    "这样可以在构建网络时更容易地预测每个图层的输出形状"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "+ 假设卷积核$k_h$是奇数，则$p_h = k_h - 1$是偶数\n",
    "我们将在高度的两侧填充$p_h / 2$行\n",
    "+ 假设卷积核$k_h$是偶数，则$p_h = k_h - 1$是奇数\n",
    "则可能性是在输入顶部填充$\\lceil p_h / 2 \\rceil$，在底部填充$\\lfloor p_h / 2 \\rfloor $。\n",
    "\n",
    "同理，我们填充宽度的两侧。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "CNN中卷积核的高度和宽度通常为奇数，例如1，3，5，7。\n",
    "这样的好处是，在保持空间维度的同时，\n",
    "我们可以在顶部和底部填充相同数量的行，在左右填充相同数量的列。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "比如，在下面的例子中，\n",
    "我们创建一个高度和宽度为3的二维卷积层，\n",
    "\n",
    "并在所有侧边填充1个像素。\n",
    "\n",
    "给定高度和宽度为8的输入，\n",
    "\n",
    "则输出的高度和宽度也是8"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([8, 8])"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "# 为了方便起见，我们定义了一个计算卷积层的函数。\n",
    "# 此函数初始化卷积层权重，并对输入和输出提高和缩减相应的维数\n",
    "\n",
    "def comp_conv2d(conv2d, X):\n",
    "    # 这里的（1，1）表示B=C=1\n",
    "    X = X.reshape((1,1)+X.shape)\n",
    "    Y = conv2d(X)\n",
    "    # 省略前两个维度，B和C\n",
    "    return Y.reshape(Y.shape[2:])\n",
    "\n",
    "# 请注意，这里每边都填充的1行或1列，因此总共添加了2行2列\n",
    "conv2d = nn.Conv2d(1,1,kernel_size=3, padding=1)\n",
    "X = torch.rand(size=(8,8))\n",
    "comp_conv2d(conv2d,X).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "当卷积核的宽和高不一致时，\n",
    "\n",
    "我们可以填充不同的宽度和高度，使输入和输出具有相同的宽度和高度。\n",
    "\n",
    "如下，我们使用高度为5，宽度为3的卷积核，高度和宽度两边的填充分别为2和1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([8, 8]), torch.Size([8, 8]))"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(1,1, kernel_size=(5,3), padding=(2,1))\n",
    "comp_conv2d(conv2d, X).shape, X.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "上方也可以看出\n",
    "$P_w = (5 -1) / 2 \\\\\n",
    "P_h = (3-1)/2\n",
    "$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 步幅"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "在计算互相关时，卷积窗口从输入张量的左上角开始，向下、向右滑动。\n",
    "在前面的例子中，我们默认每次滑动一个元素。\n",
    "但是，有时候为了高效计算或是缩减采样次数，卷积窗口可以跳过中间位置，每次滑动多个元素。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "通常，当垂直步幅为$s_h$，水平步幅为$s_w$时，输出形状为：\n",
    "$$\n",
    "\\lfloor (n_h - k_h + p_h + s_h) / s_h \\rfloor \\times \\lfloor (n_w - k_w + p_w + s_w) / s_w \\rfloor\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "假设我们设置了$p_h=k_h-1$，和$p_w = k_w -1$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "则输出形状简化为\n",
    "$$\n",
    "\\lfloor (n_h + s_h - 1) / s_h \\rfloor \\times \\lfloor (n_w + s_w - 1) / s_w\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "更进一步，如果输入的高度与宽度可以被垂直和水平步幅整除，则输出形状为\n",
    "$$\n",
    " (n_h / s_h) \\times (n_w / s_w)\n",
    "$$\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "下面，我们将高度和宽度的步幅设置为2，从而将输入的宽度和高度减半。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([4, 4])"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(1,1,kernel_size=3, padding=1, stride=2)\n",
    "comp_conv2d(conv2d, X).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "注意这里我们要填充3-1=2列，参数padding应该再除以2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 2])"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(1, 1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))\n",
    "comp_conv2d(conv2d, X).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 多输入多输出通道"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 多输入通道"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "当输入包含多个通道时，\n",
    "\n",
    "**需要构造一个与输入具有相同输入通道数的卷积核。**\n",
    "\n",
    "以便与输入数据进行互相关操作。\n",
    "\n",
    "假设输入的通道数为$c_i$，则卷积核的输入通道数也应该为$c_i$。\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "如果卷积核的窗口形状是$k_h \\times k_w$，\n",
    "那么当$c_i = 1$时，我们可以把卷积核看作形状为的二维张量。\n",
    "然而，当$c_i > 1$时，我们卷积核每个输入通道都将包含形状为$k_h \\times k_w$的张量。\n",
    "将这$c_i$个张量连结到一起可以得到形状为$c_i \\times k_h \\times k_w $的卷积核"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "由于输入和卷积核都有$c_i$个通道，\n",
    "\n",
    "我们可以对每个通道输入的二维张量和卷积核的二维张量进行互相关运算，\n",
    "再对通道求和得到二维张量。\n",
    "\n",
    "这是多通道输入和多输入通道卷积核之间进行二维互相关运算的结果。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![多通道卷积](imgs/多通道卷积.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "import torch\n",
    "from d2l import torch as d2l\n",
    "\n",
    "def corr2d_multi_in(X,K):\n",
    "    # 先遍历X和K的第0个维度（通道维度），再将它们架在一起\n",
    "    return sum(corr2d(x, k) for x, k in zip(X, K))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 56.,  72.],\n        [104., 120.]])"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]],\n",
    "               [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]])\n",
    "K = torch.tensor([[[0.0, 1.0], [2.0, 3.0]], [[1.0, 2.0], [3.0, 4.0]]])\n",
    "\n",
    "corr2d_multi_in(X, K)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 3]) torch.Size([2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, K.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 多输出通道\n",
    "到目前为止，不论有多少输入通道，我们还只有一个输出通道。\n",
    "然而，每一层有多个输出通道是至关重要的。\n",
    "\n",
    "在最流行的神经网络架构中，随着神经网络层数的加深，我们常会增加输出通道的维数，通过减少空间分辨率以获得更大的通道深度。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "直观来说，我们可以把每个通道看作对不同特征的响应。\n",
    "而现实可能会更复杂一点，因为每个通道不是独立学习的，\n",
    "而是为了共同使用而优化的。\n",
    "\n",
    "因此，多输出通道并不仅是学习多个单通道的检测器。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "用$c_i$和$c_o$分别表示输入和输出通道的数目，\n",
    "并让$k_h$和$k_w$作为卷积核的高度和宽度。\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "为了获得多个通道的输出，我们可以为\n",
    "\n",
    "每一个输出通道创建一个$c_i \\times k_h \\times k_w$的卷积核张量。\n",
    "\n",
    "这样卷积核的形状是$c_o \\times c_i \\times k_h \\times k_w$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "在互相关运算中，每个输出通道先获取所有输入通道，再以对应输出通道的卷积核计算出结果。\n",
    "\n",
    "下面，实现一个计算多通道输出的互相关函数。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "def corr2d_multi_in_out(X, K):\n",
    "    # 迭代K的第0个维度，每次都对输入X执行互相关\n",
    "    # 最后将结果都叠加到一起\n",
    "    return torch.stack([corr2d_multi_in(X, k) for k in K], 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "通过将核张量K与K+1（K中每一个元素加1）和K+2连接起来，构造了一个具有3个输出维度的卷积核。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([3, 2, 2, 2])"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = torch.stack((K, K+1, K+2), 0)\n",
    "K.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "下面，我们对输入张量X和卷积核张量K执行互相关操作。\n",
    "现在输出包含3个通道，\n",
    "\n",
    "第一个通道的结果与先前输入张量X和多输入单输出通道的结果一致。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 56.,  72.],\n         [104., 120.]],\n\n        [[ 76., 100.],\n         [148., 172.]],\n\n        [[ 96., 128.],\n         [192., 224.]]])"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d_multi_in_out(X,K)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# $1 \\times 1$卷积"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1 × 1卷积，即$k_h = k_w = 1$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "看起来似乎没有任何意义。\n",
    "毕竟，\n",
    "卷积的本质，\n",
    "\n",
    "是有效提取相邻像素之间的相关特征。\n",
    "\n",
    "但是\n",
    "\n",
    "因为使用了最小窗口，1 × 1 卷积失去了卷积层的特有能力\n",
    "\n",
    " ----- 在高度与宽度维度上，识别相邻元素间相互作用的能力。\n",
    "其实，\n",
    "1 × 1卷积的唯一计算发生在通道上。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![1x1卷积](imgs/1x1卷积.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "上图展示了使用 1 × 1卷积核 与 3个输入通道和2个输出通道的互相关操作。\n",
    "\n",
    "这里输入和输出具有相同的高度和宽度。\n",
    "\n",
    "输出中每个元素都是从输入图像中同一位置的元素的线性组合。\n",
    "\n",
    "我们可以把 1 × 1 卷积层看作在每个像素位置应用的全连接层，\n",
    "\n",
    "以$c_i$个输入值转换为$c_o$个输出值。\n",
    "\n",
    "因为这仍旧是一个卷积层，所以跨像素的权重是一致的。\n",
    "\n",
    "同时，1 × 1 卷积层需要的权重维度为$c_o \\times c_i$，再额外添加一个偏置。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "下面，我们使用全连接层实现卷积。 请注意，我们需要对输入和输出的数据形状进行调整。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "def corr2d_multi_in_out_1x1(X, K):\n",
    "    c_i, h, w = X.shape # 输入的通道，高，宽\n",
    "    c_o = K.shape[0] # 需要的输出维度\n",
    "    X = X.reshape((c_i, h * w)) # 输入转为2维度 -- 方便矩阵乘法\n",
    "    K = K.reshape((c_o,  c_i)) # 卷积核也转为2维\n",
    "    # 全连接层中的矩阵乘法\n",
    "    Y = torch.matmul(K,X)\n",
    "    return Y.reshape((c_o, h, w))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "当执行1×1卷积运算时，上述函数相当于先前实现的互相关函数corr2d_multi_in_out。让我们用一些样本数据来验证这一点"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]],\n\n        [[0., 0., 0.],\n         [0., 0., 0.],\n         [0., 0., 0.]]])"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.normal(0, 1, (3,3,3)) # 输入 通道3 高3 宽3\n",
    "K = torch.normal(0,1, (2,3,1,1)) # 卷积核 需要两个输出， 通道3 高1 宽1\n",
    "Y1 = corr2d_multi_in_out_1x1(X, K)\n",
    "Y2 = corr2d_multi_in_out(X,K)\n",
    "Y1 - Y2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 汇聚层（池化层）"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "通常当我们处理图像时，\n",
    "\n",
    "我们希望逐渐降低隐藏表示的空间分辨率、聚集信息，\n",
    "这样随着我们在神经网络中层叠的上升，每个神经元对其敏感的感受野（输入）就越大。\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. 我们的机器学习任务通常跟全局的图像有关\n",
    "2. 拍摄一个静止的物体，由于快门的移动而引起的相机振动，可能会使所有物体左右移动一个像素"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 汇聚（pooling）层，\n",
    "它具有双重目的：\n",
    "+ 降低卷积层对位置的敏感性\n",
    "+ 降低对空间降采样表示的敏感性。\n",
    "      如果我们拍摄黑白之间轮廓清晰的图像X，并将整个图像向右移动一个像素，即Z[i, j] = X[i, j + 1]，"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 最大池化\n",
    "![最大池化](imgs/最大池化.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "回到前面所说的对象边缘检测示例，\n",
    "现在我们使用卷积层的输出作为 2 × 2 最大汇聚的输入。\n",
    "\n",
    "设置卷积层输入为X，汇聚层输出为Y。\n",
    "无论X[i,j]和X[i, j+1]的值是否相同，\n",
    "或X[i,j+1]和X[i,j+2]的值是否相同，\n",
    "汇聚层始终使出Y[i,j]=1。\n",
    "\n",
    "也就是说，使用2 × 2最大汇聚层，即使在高度和宽度上移动一个元素，\n",
    "卷积层依旧可以识别到模式。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "def pool2d(X, pool_size, mode='max'):\n",
    "    p_h, p_w = pool_size\n",
    "    # 先把输入Y形状设置好\n",
    "    # 输出的形状，同样类似与卷积 N - K + 1\n",
    "    Y = torch.zeros(\n",
    "        (X.shape[0] - p_h + 1, X.shape[1] - p_w + 1)\n",
    "    )\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            if mode == 'max':\n",
    "                Y[i, j] = X[i: i+p_h, j: j+p_w].max()\n",
    "            elif mode =='avg':\n",
    "                Y[i, j ] = X[i:i+p_h, j: j+p_w].mean()\n",
    "    return Y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[4., 5.],\n        [7., 8.]])"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor(\n",
    "    [[0.0, 1.0, 2.0],\n",
    "     [3.0, 4.0, 5.0],\n",
    "     [6.0, 7.0, 8.0]]\n",
    ")\n",
    "pool2d(X,(2,2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[2., 3.],\n        [5., 6.]])"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d(X,(2,2), 'avg')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 填充与步幅"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "与卷积层一样，汇聚层也可以改变输出形状。和以前一样，我们可以通过填充和步幅以获得所需的输出形状。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[[ 0.,  1.,  2.,  3.],\n          [ 4.,  5.,  6.,  7.],\n          [ 8.,  9., 10., 11.],\n          [12., 13., 14., 15.]]]])"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(16, dtype=torch.float32).reshape((1,1,4,4))\n",
    "X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "默认情况下，深度学习框架中的\n",
    "\n",
    "**步幅与汇聚窗口的大小相同**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "因此，如果我们使用（3，3）的汇聚窗口，那么默认情况下，步幅也是(3,3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[[10.]]]])"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d = nn.MaxPool2d(3)\n",
    "pool2d(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[[ 5.,  7.],\n          [13., 15.]]]])"
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d = nn.MaxPool2d(3, padding=1, stride=2)\n",
    "pool2d(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[[ 5.,  7.],\n          [13., 15.]]]])"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d = nn.MaxPool2d((2, 3), stride=(2, 3), padding=(0, 1))\n",
    "pool2d(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 多个通道"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "在处理多通道输入数据时，汇聚层在每个输入通道上单独运算。\n",
    "而不是像卷积层一样在通道上对输入进行汇总。\n",
    "\n",
    "下面，我们在通道上连结张量X和X+1，以构建具有两个通道的输入"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 2, 4, 4])"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(16, dtype=torch.float32).reshape((1,1,4,4))\n",
    "torch.cat((X, X+1),dim=1).shape # 在通道维度拼接"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 1, 4, 4])"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((X, X+1),dim=0).shape # 在Batch维度拼接"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "X = torch.cat((X, X+1),dim=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 2, 2, 2])"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d = nn.MaxPool2d(3, padding=1, stride=2)\n",
    "pool2d(X).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LeNet"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![LeNet](imgs/LeNet.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "from torchsummary import summary\n",
    "net = nn.Sequential(\n",
    "    # 输入 1 *  28 * 28\n",
    "\n",
    "    # 第一层卷积，尺度没有发生变化，通道数增加\n",
    "    nn.Conv2d(1,6,kernel_size=5, padding=2,), # 6 * 28 * 28\n",
    "    nn.Sigmoid(),\n",
    "    nn.AvgPool2d(kernel_size=2,stride=2), # 6 * 14 * 14\n",
    "    nn.Conv2d(6,16,kernel_size=5), # 16 * (14 - 5 + 1) * (14 - 5 + 1)\n",
    "    nn.Sigmoid(), # 16 * 10 * 10\n",
    "    nn.AvgPool2d(kernel_size=2, stride=2), # 16 * floor[(10 - 2 + 1 + 2) / 2 ]* floor[(10 - 2 + 1 + 2) / 2 ]= 16 * 5 * 5\n",
    "    nn.Flatten(), # 16 * 5 * 5\n",
    "    nn.Linear(16 * 5 * 5, 120),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(120,84),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(84, 10)\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1             [1, 6, 28, 28]             156\n",
      "           Sigmoid-2             [1, 6, 28, 28]               0\n",
      "         AvgPool2d-3             [1, 6, 14, 14]               0\n",
      "            Conv2d-4            [1, 16, 10, 10]           2,416\n",
      "           Sigmoid-5            [1, 16, 10, 10]               0\n",
      "         AvgPool2d-6              [1, 16, 5, 5]               0\n",
      "           Flatten-7                   [1, 400]               0\n",
      "            Linear-8                   [1, 120]          48,120\n",
      "           Sigmoid-9                   [1, 120]               0\n",
      "           Linear-10                    [1, 84]          10,164\n",
      "          Sigmoid-11                    [1, 84]               0\n",
      "           Linear-12                    [1, 10]             850\n",
      "================================================================\n",
      "Total params: 61,706\n",
      "Trainable params: 61,706\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.11\n",
      "Params size (MB): 0.24\n",
      "Estimated Total Size (MB): 0.35\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(summary(net, input_size=(1, 28, 28), batch_size=1, device='cpu'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 参数量计算\n",
    "[卷积参数量计算](https://blog.csdn.net/weixin_38632246/article/details/103177846)\n",
    "$$(C_{in} * (K * K) + 1) * C_{out}$$\n",
    "1. Conv2d-1普通卷积\n",
    "    (1*(5*5)+1)*6 = 156"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "data": {
      "text/plain": "156"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1*(5*5)+1)*6"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "# for layer in net:\n",
    "#     X = layer(X)\n",
    "#     print(layer.__class__.__name__, 'output shape: \\t', X.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 模型训练"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "为了进行评估"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "def evaluate_accuracy_gpu(net, data_iter, device=None):\n",
    "    if isinstance(net, nn.Module):\n",
    "        net.eval() # 设置为评估模式\n",
    "        if not device:\n",
    "            device = next(iter(net.parameters())).device\n",
    "    # 正确预测的数量，总预测的数量\n",
    "    # ccumulator，\n",
    "    # 用于对多个变量进行累加。\n",
    "    # Accumulator实例中创建了2个变量， 分别用于存储正确预测的数量和预测的总数量。\n",
    "    # 当我们遍历数据集时，两者都将随着时间的推移而累加。\n",
    "    metric = d2l.Accumulator(2)\n",
    "    # p评估不需要构建计算图，没有grad_fn=<AddmmBackward>属性\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            if isinstance(X, list):\n",
    "                # BERT所需要的微调\n",
    "                X = [x.to(device) for x in X]\n",
    "            else:\n",
    "                X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            metric.add(d2l.accuracy(net(X), y), y.numel())\n",
    "    return metric[0] / metric[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# model.eval()与with torch.no_grad()的区别\n",
    "model.eval主要用在模型前向传播过程中，\n",
    "通过设置城eval模式，告诉所有层你在eval模式，其中涉及到BN和Dropout层。这些层在训练和测试的表现是不一样的, 比如 dropout 在训练中可能是0-1间的数, 但在eval模式则为不使用dropout层.\n",
    "\n",
    "torch.no_grad() 会关闭自动求导引擎的,  因此能节省显存, 和加速.\n",
    "\n",
    "总结： 如果你的网络中包含batchNorm或者dropout这样在training,eval时表现不同的层，应当使用model.eval()。在inference时用with torch.no_grad()会节省存储空间。\n",
    "\n",
    "另外需要注意的是，即便不使用with torch.no_grad()，在测试只要你不调用loss.backward()就不会计算梯度，with torch.no_grad()的作用只是节省存储空间。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "def train_ch6(net, train_iter, test_iter, num_epochs, lr, device):\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "    net.apply(init_weights)\n",
    "    print('training on: ', device)\n",
    "    net.to(device)\n",
    "\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    animator = d2l.Animator(\n",
    "        xlabel='epoch',\n",
    "        xlim=[1, num_epochs],\n",
    "        legend=['train loss', 'train acc', 'test acc']\n",
    "    )\n",
    "\n",
    "    timer, num_batches = d2l.Timer(), len(train_iter)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练损失之和\n",
    "        metric = d2l.Accumulator(3)\n",
    "        net.train()\n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "            timer.start()\n",
    "            optimizer.zero_grad()\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_hat = net(X)\n",
    "            # 这里计算的时batch_size个样本的loss均值\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                # 这里只是为了得到batch_size样本的总loss，总acc，以及batch_size\n",
    "                metric.add(l * X.shape[0], d2l.accuracy(y_hat,y), X.shape[0])\n",
    "            timer.stop()\n",
    "            train_l = metric[0] / metric[2]\n",
    "            train_acc = metric[1] / metric[2]\n",
    "            if (i + 1 ) % (num_batches // 5) == 0 or i == num_batches-1:\n",
    "                animator.add(epoch+(i+1) / num_batches,\n",
    "                             (train_l, train_acc, None))\n",
    "        test_acc = evaluate_accuracy_gpu(net, test_iter, device='cpu')\n",
    "        animator.add(epoch+1, (None, None, test_acc))\n",
    "    print(f'loss {train_l:.3f}, train acc:{train_acc:.3f}, test_acc:{test_acc:.3f}')\n",
    "    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec on {str(device)}  ')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# lr, num_epochs = 0.9, 5\n",
    "# train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}